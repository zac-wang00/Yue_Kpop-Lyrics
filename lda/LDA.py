import pandas as pd
from gensim import corpora
from gensim.models import LdaModel
from matplotlib.font_manager import FontProperties
import re
import ast # å¼•å…¥ Abstract Syntax Tree æ¨¡çµ„
import numpy as np
from tqdm import tqdm

input_file = "data/lyrics_processed_with_tokens.csv"
#company = "SM"
df = pd.read_csv(input_file)
# ====================================================================
# âš ï¸ 1. è³‡æ–™è®€å–èˆ‡æº–å‚™ (è«‹æ ¹æ“šä½ çš„å¯¦éš›ç¨‹å¼ç¢¼ä¿®æ”¹é€™éƒ¨åˆ†)
# ====================================================================

# å‡è¨­ä½ çš„ DataFrame å·²ç¶“è¼‰å…¥ï¼Œä¸¦ä¸”å·²ç¶“å®Œæˆäº†æ‰€æœ‰çš„é è™•ç†å’Œåˆä½µæ­¥é©Ÿ
# ä¾‹å¦‚ï¼š df = pd.read_csv('your_data.csv')
# å‡è¨­ df['final_tokens'] æ¬„ä½æ˜¯ List of Strings é¡å‹

# ç¢ºä¿ 'final_tokens' æ¬„ä½ä¸­çš„æ¯å€‹å­—ä¸²éƒ½è¢«å®‰å…¨åœ°è©•ä¼°ç‚º Python åˆ—è¡¨
def convert_str_to_list(list_str):
    try:
        # ast.literal_eval æ¯” eval() æ›´å®‰å…¨ï¼Œå°ˆé–€ç”¨æ–¼è©•ä¼°å­—ä¸²ä¸­çš„åŸºæœ¬æ•¸æ“šçµæ§‹
        return ast.literal_eval(list_str)
    except (ValueError, TypeError):
        # å¦‚æœé‡åˆ° NaN æˆ–ç„¡æ³•è©•ä¼°çš„å­—ä¸²ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []



# æ‡‰ç”¨è½‰æ›ï¼Œé€™å°‡æ˜¯ä½ çš„æ–°æœ€çµ‚è©å½™æ¬„ä½
df['final_tokens_restored'] = df['final_tokens'].apply(convert_str_to_list)
documents = df['final_tokens_restored'].tolist()

#df = df[df['label name'] == company]
#documents = df['final_tokens_restored'].tolist()
# --------------------------------------------------------------------
# æ¥ä¸‹ä¾†çš„ LDA æµç¨‹ï¼Œè«‹ä½¿ç”¨é€™å€‹æ–°çš„é‚„åŸæ¬„ä½
# --------------------------------------------------------------------
# 1. å»ºç«‹æ–‡æª”é›†åˆ (List of Lists)

# 2. ç§»é™¤ç©ºæ–‡æª”ï¼ˆå®‰å…¨æ“ä½œï¼‰
documents = [doc for doc in documents if doc]
# ====================================================================
# 2. æ•¸æ“šé©—è­‰èˆ‡å®‰å…¨æª¢æŸ¥ (é¿å… ValueError: cannot compute LDA over an empty collection)
# ====================================================================
total_docs = len(documents)
empty_docs_count = sum(1 for doc in documents if not doc)
total_tokens = sum(len(doc) for doc in documents)

print("\n--- æ•¸æ“šæµå¤±æœ€çµ‚æª¢æŸ¥ ---")
print(f"æ–‡æª”ç¸½æ•¸ (æ­Œæ›²æ•¸): {total_docs}")
print(f"ç©ºåˆ—è¡¨æ–‡æª”æ•¸: {empty_docs_count}")
print(f"æ‰€æœ‰æ–‡æª”ä¸­è©å½™çš„ç¸½è¨ˆæ•¸: {total_tokens}")

if total_tokens == 0:
    print("ğŸš¨ è‡´å‘½éŒ¯èª¤ï¼šæ‰€æœ‰æ–‡æª”è©å½™ç¸½è¨ˆæ•¸ç‚º 0ã€‚è«‹æª¢æŸ¥ DataFrame åŸå§‹æ¬„ä½ã€‚")
    exit()  # åœæ­¢åŸ·è¡Œ

# --------------------------------------------------------------------
# ç§»é™¤ç©ºæ–‡æª”ï¼ˆå¦‚æœç©ºæ–‡æª”æ•¸é‡ä¸å¤šï¼Œé€™æ¨£å¯ä»¥é¿å…å®ƒå€‘å¹²æ“¾å¾ŒçºŒè™•ç†ï¼‰
documents = [doc for doc in documents if doc]
# --------------------------------------------------------------------


# ====================================================================
# 3. å»ºç«‹è©å…¸ (Dictionary) å’Œèªæ–™åº« (Corpus)
# ====================================================================

print("\né–‹å§‹å»ºç«‹è©å…¸...")
# ä½¿ç”¨æ‰€æœ‰æ–‡æª”å»ºç«‹è©å…¸
dictionary = corpora.Dictionary(documents)

# è©å½™éæ¿¾ï¼šä½¿ç”¨æœ€å¯¬é¬†çš„æ¢ä»¶ä¾†é¿å…ä¸Ÿå¤±æ ¸å¿ƒè©
dictionary.filter_extremes(
    no_below=10,  # è©å½™è‡³å°‘åœ¨ 2 é¦–æ­Œä¸­å‡ºç¾é
    #no_above=0.99,  # è©å½™åªæœ‰åœ¨è¶…é 99% çš„æ­Œä¸­å‡ºç¾æ‰ç§»é™¤
    keep_n=None
)

print(f"âœ… è©å½™è¡¨å¤§å° (éæ¿¾å¾Œ): {len(dictionary)}")

# å»ºç«‹ BoW èªæ–™åº« (å°‡è©å½™è½‰æ›ç‚º (ID, Count) æ ¼å¼)
corpus = [dictionary.doc2bow(doc) for doc in documents]
print(f"âœ… èªæ–™åº«æ–‡æª”æ•¸: {len(corpus)}")

# ====================================================================
# 4. è¨“ç·´ LDA æ¨¡å‹ (LdaModel)
# ====================================================================

# âš ï¸ é—œéµåƒæ•¸ï¼š num_topics (å»ºè­°å¾ 10 é–‹å§‹å˜—è©¦)
NUM_TOPICS = 6

print(f"\né–‹å§‹è¨“ç·´ {NUM_TOPICS} å€‹ä¸»é¡Œçš„ LDA æ¨¡å‹...")
# ç¨€ç– Alpha: é¼“å‹µæ¯é¦–æ­Œåªå°ˆæ³¨æ–¼å°‘æ•¸ä¸»é¡Œ
EXPERIMENTAL_ALPHA = 0.01

# ç¨€ç– Eta: é¼“å‹µæ¯å€‹ä¸»é¡Œåªç”±å°‘æ•¸é—œéµè©çµ„æˆ (0.1 æ˜¯å¸¸è¦‹çš„ç¨€ç–å€¼)
EXPERIMENTAL_ETA = 0.1

lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=NUM_TOPICS,
    random_state=42,  # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
    chunksize=100,
    passes=20,  # å¢åŠ è¿­ä»£æ¬¡æ•¸ä»¥æé«˜æ¨¡å‹å“è³ª
    alpha=EXPERIMENTAL_ALPHA,
    eta=EXPERIMENTAL_ETA
)

print("âœ… LDA æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

# ====================================================================
# 5. çµæœè§£è®€ï¼šæª¢è¦–ä¸»é¡Œ (ä½¿ç”¨ CJK å­—å‹ç¢ºä¿éŸ“æ–‡é¡¯ç¤º)
# ====================================================================

# âš ï¸ è«‹ç¢ºä¿ä½ çš„ FONT_PATH æŒ‡å‘ä¸€å€‹æ”¯æŒéŸ“æ–‡çš„å­—å‹ï¼Œä¾‹å¦‚ Malgun Gothic
FONT_PATH = 'C:\\Windows\\Fonts\\malgun.ttf'
try:
    cjk_font = FontProperties(fname=FONT_PATH)
except:
    print("âš ï¸ è­¦å‘Šï¼šç„¡æ³•è¼‰å…¥éŸ“æ–‡å­—å‹ã€‚çµ‚ç«¯æ©Ÿè¼¸å‡ºå¯èƒ½æœƒå‡ºç¾äº‚ç¢¼ã€‚")

print("\n--- LDA ä¸»é¡Œæ¨¡å‹çµæœ (Top 10 è©å½™) ---")

for idx, topic in lda_model.print_topics(num_words=10):
    # æ¸…ç†è¼¸å‡ºæ ¼å¼ï¼šç§»é™¤æ•¸å­—æ¬Šé‡å’Œå°æ•¸é»ï¼Œåªä¿ç•™è©å½™
    # ç¯„ä¾‹è¼¸å‡º: 0.050*"word" + 0.040*"word2"
    cleaned_topic = re.sub(r'\d\.\d{3}\*"', '', topic).replace('"', '').replace(' + ', ' / ')

    # æ‰“å°çµæœ (å¦‚æœçµ‚ç«¯æ©Ÿæ”¯æŒï¼ŒéŸ“æ–‡æœƒæ­£å¸¸é¡¯ç¤º)
    print(f"ğŸŒŸ ä¸»é¡Œ #{idx + 1}ï¼š")
    print(f"   {cleaned_topic}\n")

## ------------------------------------------------------------------
## è¼¸å‡º 1: ä¸»é¡Œ-è©å½™æ¦‚ç‡ (phi)
## ------------------------------------------------------------------

print("==============================================")
print(f"âœ¨ ä¸»é¡Œ-è©å½™æ¦‚ç‡ (Top 15 è©å½™ï¼Œå…± {NUM_TOPICS} å€‹ä¸»é¡Œ)")
print("==============================================")

# ç²å–æ¯å€‹ä¸»é¡Œçš„ Top è©å½™å’Œæ¬Šé‡
topics_and_probs = lda_model.show_topics(
    num_topics=NUM_TOPICS,
    num_words=15,  # é€™è£¡æˆ‘å€‘æå– Top 15 è©å½™ï¼Œæ¯”ä½ ä¹‹å‰çµ¦çš„ Top 10 æ›´è©³ç´°
    formatted=False
)

for topic_id, word_probs in topics_and_probs:
    print(f"\nğŸŒŸ ä¸»é¡Œ #{topic_id + 1}:")
    # å°‡è©å½™åŠå…¶æ¦‚ç‡æ ¼å¼åŒ–è¼¸å‡º
    output_str = ", ".join([f"{word} ({prob:.4f})" for word, prob in word_probs])
    print(output_str)

## ------------------------------------------------------------------
## è¼¸å‡º 2: æ–‡æª”-ä¸»é¡Œæ¦‚ç‡ (theta) - æ–°å¢æ¬„ä½
## ------------------------------------------------------------------

print("\n==============================================")
print("ğŸ“„ æ–‡æª”-ä¸»é¡Œæ¦‚ç‡ (æ–°å¢ä¸»é¡Œåˆ†ä½ˆæ¬„ä½)")
print("==============================================")

# ä½¿ç”¨ lda_model.get_document_topics() ç¢ºä¿è¼¸å‡ºæ˜¯ç¨€ç–æ ¼å¼çš„ (topic_id, probability) å…ƒçµ„åˆ—è¡¨
# å¿…é ˆå‚³å…¥ corpus åƒæ•¸ä½œç‚ºè¼¸å…¥ã€‚
doc_topics = [
    lda_model.get_document_topics(
        doc,
        minimum_probability=0.0
    )
    for doc in corpus
]
# ----------------------------------------------------------------------

# 2. æ ¼å¼åŒ–ç‚º DataFrame çµæ§‹ (ä¿æŒä¸è®Š)
# æˆ‘å€‘éœ€è¦ä¸€å€‹å‡½æ•¸ä¾†å°‡ (Topic_ID, Probability) åˆ—è¡¨è½‰æ›ç‚ºå›ºå®šé•·åº¦çš„æ¦‚ç‡åˆ—è¡¨
def format_topic_distribution(topic_list, num_topics):
    """å°‡ä¸»é¡Œæ¦‚ç‡åˆ—è¡¨è½‰æ›ç‚ºå›ºå®šé•·åº¦ï¼ˆ1åˆ°Nï¼‰çš„æ¦‚ç‡å‘é‡"""

    # å‰µå»ºä¸€å€‹é•·åº¦ç‚º num_topics çš„é›¶å‘é‡
    prob_vector = np.zeros(num_topics)

    # å°‡ä¸»é¡Œåˆ—è¡¨ä¸­çš„æ¦‚ç‡å¡«å…¥å°æ‡‰çš„ä½ç½®
    for topic_id, prob in topic_list:
        if topic_id < num_topics:
            prob_vector[topic_id] = prob

    return prob_vector.tolist()


# 3. æ‡‰ç”¨æ ¼å¼åŒ–å‡½æ•¸
# é€™è£¡æ‡‰è©²å¯ä»¥æˆåŠŸåŸ·è¡Œï¼Œå› ç‚º doc_topics å·²ç¶“æ˜¯é æœŸçš„ (id, prob) åˆ—è¡¨
topic_distributions = [format_topic_distribution(doc, NUM_TOPICS) for doc in doc_topics]

# 4. å‰µå»ºæ–°çš„ DataFrame æ¬„ä½åç¨± (ä¿æŒä¸è®Š)
topic_cols = [f'Topic_{i + 1}_Prob' for i in range(NUM_TOPICS)]

# 5. å°‡ä¸»é¡Œæ¦‚ç‡æ·»åŠ åˆ°ä½ çš„åŸå§‹ DataFrame (df) (ä¿æŒä¸è®Š)
df_topic_probs = pd.DataFrame(topic_distributions, columns=topic_cols)

# ç¢ºä¿ df å’Œ df_topic_probs çš„é•·åº¦ä¸€è‡´
# âš ï¸ ç”±æ–¼ä½ çš„åŸå§‹ä»£ç¢¼ä¸­ df çš„ä¾†æºå’Œ 'corpus' çš„è™•ç†æ²’æœ‰å®Œå…¨é¡¯ç¤ºï¼Œ
# é€™è£¡æˆ‘å€‘å‡è¨­å®ƒå€‘æ˜¯åŒæ­¥çš„ã€‚
df = pd.concat([df.reset_index(drop=True), df_topic_probs], axis=1)

print(f"âœ… å·²æˆåŠŸç‚º DataFrame æ–°å¢ {NUM_TOPICS} å€‹ä¸»é¡Œæ¦‚ç‡æ¬„ä½ã€‚")
print("\n--- å¸¶æœ‰ä¸»é¡Œæ¦‚ç‡çš„ DataFrame å‰ 5 è¡Œ ---")
print(df[topic_cols].head())
# ------------------------------------------------------------------
# ã€ä¿®æ­£ï¼šè™•ç† NaN å€¼ä»¥é¿å… FutureWarningã€‘
# ------------------------------------------------------------------

# 1. å®šç¾©ä¸»é¡Œæ¦‚ç‡æ¬„ä½
topic_cols = [f'Topic_{i + 1}_Prob' for i in range(NUM_TOPICS)]

# 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨è¨ˆç®—æœ€å¤§å€¼ä¹‹å‰ï¼Œå°‡æ‰€æœ‰ NaN æ›¿æ›ç‚º 0.0
# é€™èƒ½ç¢ºä¿ idxmax ç¸½èƒ½æ‰¾åˆ°ä¸€å€‹æœ€å¤§å€¼ (å³ä½¿å®ƒæ˜¯ 0.0)
df[topic_cols] = df[topic_cols].fillna(0.0)

# 3. æ‰¾å‡ºæ¯è¡Œ (æ¯é¦–æ­Œ) çš„æœ€å¤§æ¦‚ç‡å€¼å’Œä¸»å°ä¸»é¡Œ
# ä¿®æ­£å¾Œçš„ä»£ç¢¼å°‡ä¸å†è§¸ç™¼ FutureWarning
df['Dominant_Topic_Prob'] = df[topic_cols].max(axis=1)  # æœ€å¤§æ¦‚ç‡å€¼
df['Dominant_Topic'] = df[topic_cols].idxmax(axis=1)  # æœ€å¤§æ¦‚ç‡å€¼æ‰€åœ¨æ¬„ä½åç¨± (ä¾‹å¦‚ 'Topic_2_Prob')

# 4. æ¸…ç†æ¬„ä½åç¨± (ç§»é™¤'_Prob' å’Œ 'Topic_'ï¼Œä¸¦è½‰æ›ç‚ºæ•´æ•¸)
# æ­¤æ­¥é©Ÿå¯¦ç¾æ‚¨æƒ³è¦çš„çµæœï¼šæ¬„ä½åªä¿ç•™ 1 åˆ° 10 çš„æ•¸å­—
df['Dominant_Topic_index'] = (
    df['Dominant_Topic']
    .str.replace('_Prob', '') # ç§»é™¤ '_Prob' -> 'Topic_X'
    .str.replace('Topic_', '') # ç§»é™¤ 'Topic_' -> 'X'
    .astype(int)              # è½‰æ›ç‚ºæ•´æ•¸
)

df['Dominant_Topic'] = df['Dominant_Topic'].str.replace('_Prob', '') # ç§»é™¤ '_Prob' -> 'Topic_X'

# ------------------------------------------------------------------
# ã€ä¿®æ­£/æ–°å¢ï¼šæ‰¾å‡º Top 3 ä¸»å°ä¸»é¡ŒåŠå…¶ç´¢å¼•ã€‘
# ------------------------------------------------------------------

# 1. å®šç¾©ä¸»é¡Œæ¦‚ç‡æ¬„ä½
topic_cols = [f'Topic_{i + 1}_Prob' for i in range(NUM_TOPICS)]

# 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨è¨ˆç®—æœ€å¤§å€¼ä¹‹å‰ï¼Œå°‡æ‰€æœ‰ NaN æ›¿æ›ç‚º 0.0
df[topic_cols] = df[topic_cols].fillna(0.0)


# 3. æ‰¾å‡º Top 3 ä¸»é¡ŒåŠå…¶æ¦‚ç‡å’Œç´¢å¼•
def get_top_n_topics(row, n=3):
    """å¾æ¦‚ç‡æ¬„ä½ä¸­æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„å‰ N å€‹ä¸»é¡Œçš„åç¨±ã€å€¼å’Œç´¢å¼•ã€‚"""
    # é¸æ“‡æ‰€æœ‰ä¸»é¡Œæ¦‚ç‡æ¬„ä½ï¼Œä¸¦å°‡çµæœæ’åº
    sorted_probs = row[topic_cols].sort_values(ascending=False).head(n)

    results = {}
    for rank in range(n):
        topic_key = f'Top{rank + 1}'

        if rank < len(sorted_probs):
            # ç²å–ç¬¬ rank+1 åçš„ä¸»é¡Œæ¬„ä½åç¨± (e.g., 'Topic_X_Prob')
            topic_col_name = sorted_probs.index[rank]

            # ç²å–ä¸»é¡Œæ¦‚ç‡å€¼
            prob_value = sorted_probs.iloc[rank]

            # æå–ç´”ä¸»é¡Œåç¨± (e.g., 'Topic_X')
            topic_name = topic_col_name.replace('_Prob', '')

            # æå–ä¸»é¡Œç´¢å¼• (e.g., X)
            topic_index = int(topic_name.replace('Topic_', ''))

            results[f'{topic_key}_Topic'] = topic_name
            results[f'{topic_key}_Prob'] = prob_value
            results[f'{topic_key}_Topic_Index'] = topic_index  # <-- æ–°å¢ç´¢å¼•æ¬„ä½
        else:
            # å¦‚æœä¸»é¡Œæ•¸å°‘æ–¼ Nï¼Œå‰‡å¡«å…¥é è¨­å€¼
            results[f'{topic_key}_Topic'] = 'N/A'
            results[f'{topic_key}_Prob'] = 0.0
            results[f'{topic_key}_Topic_Index'] = 0  # <-- N/A ç´¢å¼•è¨­ç‚º 0

    return pd.Series(results)


# æ‡‰ç”¨æ­¤å‡½æ•¸åˆ° DataFrame çš„æ¯ä¸€è¡Œ
df_top_topics = df.apply(get_top_n_topics, axis=1)

# å°‡ Top 3 çµæœèˆ‡åŸå§‹ DataFrame åˆä½µ
# æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ä½¿ç”¨ errors='ignore' ä¾†å®‰å…¨åœ°åˆªé™¤èˆŠçš„ Dominant æ¬„ä½
df = pd.concat([df.drop(columns=['Dominant_Topic', 'Dominant_Topic_Prob', 'Dominant_Topic_index'], errors='ignore'),
                df_top_topics], axis=1)

# å°‡ Top 1 è¦–ç‚º Dominant Topic (èˆ‡èˆŠæ¬„ä½ä¿æŒä¸€è‡´)
df['Dominant_Topic'] = df['Top1_Topic']
df['Dominant_Topic_Prob'] = df['Top1_Prob']
df['Dominant_Topic_index'] = df['Top1_Topic_Index']  # <-- ç›´æ¥ä½¿ç”¨ Top1_Topic_Index


# ------------------------------------------------------------------
# ------------------------------------------------------------------
# ã€æ–°å¢æ›¿æ›ï¼šæœ€ä»£è¡¨æ€§æ–‡æª”æª¢è¦–ã€‘
# ... (display_representative_documents å‡½å¼ä¿æŒä¸è®Š) ...

def display_representative_documents(df, num_topics, top_n=5):
    """
    å°æ¯å€‹ä¸»é¡Œï¼Œæ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„ Top N é¦–æ­Œ (å³æœ€èƒ½ä»£è¡¨è©²ä¸»é¡Œçš„æ–‡æª”)ã€‚
    ç¾åœ¨ä½¿ç”¨ Top1_Topic æ¬„ä½é€²è¡Œç¯©é¸ã€‚
    """
    print("\n==============================================")
    print("ğŸ‘‘ æœ€ä»£è¡¨æ€§æ–‡æª”æª¢è¦– (Top 5 æ­Œæ›²/æ–‡æª”)")
    print("==============================================")

    for i in range(1, num_topics + 1):
        topic_name = f'Topic_{i}'

        # ç¯©é¸å‡ºä»¥ç•¶å‰ä¸»é¡Œç‚ºä¸»å°ä¸»é¡Œçš„æ­Œæ›² (ä½¿ç”¨ Top1_Topic)
        topic_subset = df[df['Top1_Topic'] == topic_name]

        if topic_subset.empty:
            print(f"ä¸»é¡Œ #{i} ({topic_name})ï¼šæ²’æœ‰ä¸»å°æ­Œæ›²ã€‚")
            continue

        # æ ¹æ“š Top1_Prob é™åºæ’åºï¼Œé¸å‡º Top N
        top_documents = topic_subset.sort_values(
            by='Top1_Prob', # é€™è£¡ä½¿ç”¨ Top1_Prob
            ascending=False
        ).head(top_n)

        print(f"\n--- ä¸»é¡Œ #{i} ({topic_name}) ---")

        # æ‰“å° Top N æ­Œæ›²è³‡è¨Š
        for index, row in top_documents.iterrows():
            prob = row['Top1_Prob'] # ä½¿ç”¨ Top1_Prob
            artist = row.get('recording_artist_credit', 'N/A')
            title = row.get('recording_title', 'N/A')

            print(f"[{prob:.4f}] {artist} - ã€Š{title}ã€‹")

# èª¿ç”¨æ–°çš„æª¢è¦–å‡½æ•¸
display_representative_documents(df, NUM_TOPICS, top_n=5)

# ------------------------------------------------------------------
# æœ€çµ‚æ‰“å° (å¯é¸ï¼Œä½œç‚ºä¸€å€‹ç¸½çµ)
# ------------------------------------------------------------------
print("\n--- æ­Œæ›²ä¸»å°ä¸»é¡Œæ­¸é¡çµæœ (å‰ 5 ç­†) ---")
# ç¢ºä¿ä½¿ç”¨ df ä¸­çš„å¯¦éš›æ¬„ä½å
print(df[['recording_artist_credit', 'recording_title', 'Dominant_Topic', 'Dominant_Topic_Prob']].head())
#import pyLDAvis.gensim_models
#import pyLDAvis

# åˆªé™¤ pyLDAvis.enable_notebook()ï¼Œå› ç‚ºæ‚¨åœ¨é Notebook ç’°å¢ƒä¸‹é‹è¡Œ
# pyLDAvis.enable_notebook()

# æº–å‚™å¯è¦–åŒ–æ•¸æ“š
#data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)

# ä¿å­˜ç‚º HTML æ–‡ä»¶
# æ–‡ä»¶åå°‡æ˜¯ "5_topic_model.html" (å¦‚æœ num_topics=5)
#pyLDAvis.save_html(data, f"./{NUM_TOPICS}_topic_model.html")

#print(f"âœ… å¯è¦–åŒ–åœ–è¡¨å·²æˆåŠŸä¿å­˜ç‚ºï¼š{NUM_TOPICS}_topic_model.html")
#print("è«‹åœ¨æ‚¨çš„ç€è¦½å™¨ä¸­æ‰“é–‹æ­¤æ–‡ä»¶ä¾†æŸ¥çœ‹çµæœã€‚")



from wordcloud import WordCloud
import matplotlib.pyplot as plt


def generate_wordcloud(counts, font_path, filename):
    """æ ¹æ“šè©é »å­—å…¸ç”¢ç”Ÿæ–‡å­—é›²ä¸¦å„²å­˜"""
    if not counts:
        print(f"æ²’æœ‰è¶³å¤ çš„è©å½™ä¾†ç”¢ç”Ÿ {filename}ã€‚")
        return

    # è¨­ç½® WordCloud åƒæ•¸
    wc = WordCloud(
        font_path=font_path,  # ä½¿ç”¨éŸ“æ–‡å­—å‹
        width=1000,
        height=600,
        background_color='white',
        max_words=200,
        prefer_horizontal=0.9  # ç›¡é‡è®“æ–‡å­—æ°´å¹³é¡¯ç¤º
    ).generate_from_frequencies(counts)

    plt.figure(figsize=(12, 8))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(filename.replace(".png", ""), fontsize=20)
    plt.savefig(filename)
    print(f"æ–‡å­—é›²å·²å„²å­˜è‡³: {filename}")
    plt.close()


# --------------------------------------------------------------------
# åŸ·è¡Œæ–‡å­—é›²ç”Ÿæˆ
# --------------------------------------------------------------------

print("\n==============================================")
print("â˜ï¸ ä¸»é¡Œè©å½™æ–‡å­—é›²ç”Ÿæˆ")
print("==============================================")

# ç²å–æ‰€æœ‰ä¸»é¡Œçš„ Top è©å½™å’Œæ¬Šé‡ (é€™è£¡ä½¿ç”¨ä½ å‰é¢æå–çš„æ•¸æ“š)
# é€™è£¡æˆ‘å€‘ä½¿ç”¨ Top 50 è©å½™ä»¥ç²å¾—æ›´è±å¯Œçš„æ–‡å­—é›²
topics_and_probs = lda_model.show_topics(
    num_topics=NUM_TOPICS,
    num_words=100,  # å¢åŠ è©å½™é‡ä»¥è±å¯Œè¦–è¦ºæ•ˆæœ
    formatted=False
)

for topic_id, word_probs in topics_and_probs:
    # å°‡ word_probs (List of Tuples) è½‰æ›ç‚º WordCloud éœ€è¦çš„å­—å…¸æ ¼å¼ {word: probability}
    word_freq_dict = dict(word_probs)

    filename = f"Topic_{topic_id + 1}_Wordcloud.png"

    # èª¿ç”¨æ–‡å­—é›²ç”Ÿæˆå‡½å¼
    generate_wordcloud(
        counts=word_freq_dict,
        font_path=FONT_PATH,  # ç¢ºä¿é€™è£¡ä½¿ç”¨ä½ å®šç¾©çš„ FONT_PATH
        filename=filename
    )


#output_file = f"LDA_topic{NUM_TOPICS}_{company}.csv"
output_file = f"LDA_topic{NUM_TOPICS}_kpop.csv"
df.to_csv(output_file, index=False, encoding='utf-8-sig')

import os

# å®šç¾©ä¿å­˜æª”æ¡ˆè·¯å¾‘
MODEL_DIR = "lda_model_assets"
if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)

lda_model_path = os.path.join(MODEL_DIR, f"lda_kpop_{NUM_TOPICS}_topics.model")
dictionary_path = os.path.join(MODEL_DIR, f"lda_kpop_{NUM_TOPICS}_dictionary.dict")

# 1. ä¿å­˜ LDA æ¨¡å‹ (ä½¿ç”¨ Gensim å…§å»ºçš„ save æ–¹æ³•)
lda_model.save(lda_model_path)

# 2. ä¿å­˜è©å…¸ (é€™æ˜¯å°‡æ–°æ–‡æª”è½‰æ›ç‚º BoW æ ¼å¼æ‰€å¿…éœ€çš„)
dictionary.save(dictionary_path)

print(f"\nâœ… LDA æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{lda_model_path}")
print(f"âœ… è©å…¸å·²ä¿å­˜è‡³ï¼š{dictionary_path}")