import matplotlib.pyplot as plt
from gensim.models.coherencemodel import CoherenceModel
import pandas as pd
from gensim import corpora
from gensim.models import LdaModel
import ast # å¼•å…¥ Abstract Syntax Tree æ¨¡çµ„
from tqdm import tqdm


def convert_str_to_list(list_str):
    try:
        # ast.literal_eval æ¯” eval() æ›´å®‰å…¨ï¼Œå°ˆé–€ç”¨æ–¼è©•ä¼°å­—ä¸²ä¸­çš„åŸºæœ¬æ•¸æ“šçµæ§‹
        return ast.literal_eval(list_str)
    except (ValueError, TypeError):
        # å¦‚æœé‡åˆ° NaN æˆ–ç„¡æ³•è©•ä¼°çš„å­—ä¸²ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []

def compute_coherence_values(dictionary, corpus, texts, topic_range):
    """
    è¨ˆç®—çµ¦å®šä¸»é¡Œæ•¸é‡ç¯„åœä¸‹çš„ Coherence Score
    """
    coherence_values = []

    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    for num_topics in tqdm(topic_range, desc="è¨ˆç®— Coherence Score"):
        # æ ¸å¿ƒä¿®æ­£ï¼šå°‡ passes é™è‡³æœ€ä½å®‰å…¨å€¼
        # é™ä½è¿­ä»£æ¬¡æ•¸ï¼Œä»¥æ¥µå¤§åŠ é€Ÿå–®æ¬¡æ¨¡å‹è¨“ç·´
        MIN_PASSES = 20

        # æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨è¼ƒå°çš„ chunksize æ¸›å°‘è¨˜æ†¶é«”å£“åŠ›
        MIN_CHUNKSIZE = 100
        # è¨“ç·´ LDA æ¨¡å‹ (ä½¿ç”¨èˆ‡ä¹‹å‰ç›¸åŒçš„åŸºç¤åƒæ•¸)
        lda_model = LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=42,
            chunksize=MIN_CHUNKSIZE,
            passes=MIN_PASSES,
            alpha='auto'
        )

        # è¨ˆç®— C_v Coherence Score
        coherence_model = CoherenceModel(
            model=lda_model,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
            #topn=10
            #workers=1

        )
        coherence_values.append(coherence_model.get_coherence())

    return coherence_values



if __name__ == '__main__':
    # è³‡æ–™è®€å–å’Œé è™•ç†
    input_file = "merged_lyrics_with_labels.csv"
    df = pd.read_csv(input_file)
    # æ‡‰ç”¨è½‰æ›ï¼Œé€™å°‡æ˜¯ä½ çš„æ–°æœ€çµ‚è©å½™æ¬„ä½
    df['final_tokens_restored'] = df['final_tokens'].apply(convert_str_to_list)
    documents = df['final_tokens_restored'].tolist()

    # ç§»é™¤ç©ºæ–‡æª”ï¼ˆå®‰å…¨æ“ä½œï¼‰
    documents = [doc for doc in documents if doc]
    # ====================================================================
    # æ•¸æ“šé©—è­‰èˆ‡å®‰å…¨æª¢æŸ¥ (é¿å… ValueError: cannot compute LDA over an empty collection)
    # ====================================================================
    total_docs = len(documents)
    empty_docs_count = sum(1 for doc in documents if not doc)
    total_tokens = sum(len(doc) for doc in documents)

    print("\n--- æ•¸æ“šæµå¤±æœ€çµ‚æª¢æŸ¥ ---")
    print(f"æ–‡æª”ç¸½æ•¸ (æ­Œæ›²æ•¸): {total_docs}")
    print(f"ç©ºåˆ—è¡¨æ–‡æª”æ•¸: {empty_docs_count}")
    print(f"æ‰€æœ‰æ–‡æª”ä¸­è©å½™çš„ç¸½è¨ˆæ•¸: {total_tokens}")

    if total_tokens == 0:
        print("ğŸš¨ è‡´å‘½éŒ¯èª¤ï¼šæ‰€æœ‰æ–‡æª”è©å½™ç¸½è¨ˆæ•¸ç‚º 0ã€‚è«‹æª¢æŸ¥ DataFrame åŸå§‹æ¬„ä½ã€‚")
        exit()  # åœæ­¢åŸ·è¡Œ

    # --------------------------------------------------------------------
    # ç§»é™¤ç©ºæ–‡æª”ï¼ˆå¦‚æœç©ºæ–‡æª”æ•¸é‡ä¸å¤šï¼Œé€™æ¨£å¯ä»¥é¿å…å®ƒå€‘å¹²æ“¾å¾ŒçºŒè™•ç†ï¼‰
    documents = [doc for doc in documents if doc]
    # --------------------------------------------------------------------

    # ====================================================================
    # å»ºç«‹è©å…¸ (Dictionary) å’Œèªæ–™åº« (Corpus)
    # ====================================================================

    print("\né–‹å§‹å»ºç«‹è©å…¸...")
    # ä½¿ç”¨æ‰€æœ‰æ–‡æª”å»ºç«‹è©å…¸
    dictionary = corpora.Dictionary(documents)

    # è©å½™éæ¿¾ï¼šä½¿ç”¨æœ€å¯¬é¬†çš„æ¢ä»¶ä¾†é¿å…ä¸Ÿå¤±æ ¸å¿ƒè©
    dictionary.filter_extremes(
        no_below=2,  # è©å½™è‡³å°‘åœ¨ 2 é¦–æ­Œä¸­å‡ºç¾é
        no_above=0.99,  # è©å½™åªæœ‰åœ¨è¶…é 99% çš„æ­Œä¸­å‡ºç¾æ‰ç§»é™¤
        keep_n=None
    )

    print(f"âœ… è©å½™è¡¨å¤§å° (éæ¿¾å¾Œ): {len(dictionary)}")

    # å»ºç«‹ BoW èªæ–™åº« (å°‡è©å½™è½‰æ›ç‚º (ID, Count) æ ¼å¼)
    corpus = [dictionary.doc2bow(doc) for doc in documents]
    print(f"âœ… èªæ–™åº«æ–‡æª”æ•¸: {len(corpus)}")

    # 3. å®šç¾©ä¸»é¡Œç¯„åœ
    min_topics = 2
    max_topics = 15
    step = 1
    topic_range = range(min_topics, max_topics + 1, step)

    # 4. åŸ·è¡Œ Coherence Score è¨ˆç®— (å ±éŒ¯çš„ç¨‹å¼ç¢¼è¡Œç¾åœ¨è¢«ä¿è­·äº†)
    coherence_scores = compute_coherence_values(
        dictionary=dictionary,
        corpus=corpus,
        texts=documents,
        topic_range=topic_range
    )
    # --------------------------------------------------------------------
    # ç¹ªåœ–å’Œé¸æ“‡æœ€ä½³ä¸»é¡Œæ•¸é‡
    # --------------------------------------------------------------------

    # å°‹æ‰¾ Coherence Score æœ€é«˜çš„é»
    max_score = max(coherence_scores)
    optimal_topic_index = coherence_scores.index(max_score)
    optimal_num_topics = topic_range[optimal_topic_index]

    # ç¹ªè£½åœ–è¡¨
    plt.figure(figsize=(10, 6))
    plt.plot(topic_range, coherence_scores, marker='o', linestyle='-', color='skyblue')

    # æ¨™è¨˜æœ€ä½³ä¸»é¡Œæ•¸é‡
    plt.scatter(optimal_num_topics, max_score, color='red', s=100,
                label=f'Best Number of Topics: {optimal_num_topics} (Score: {max_score:.4f})')
    plt.axvline(x=optimal_num_topics, color='r', linestyle='--', linewidth=0.8)

    # è¨­å®šåœ–è¡¨æ¨™ç±¤
    plt.title("LDA Coherence Score", fontsize=16)
    plt.xlabel("Number of Topics", fontsize=12)
    plt.ylabel("Coherence Score ($C_v$)", fontsize=12)
    plt.xticks(topic_range)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.tight_layout()

    # é¡¯ç¤ºçµæœ
    print("\n--- ä¸»é¡Œé€£è²«æ€§åˆ†æ•¸çµæœ ---")
    for num_topics, score in zip(topic_range, coherence_scores):
        print(f"ä¸»é¡Œæ•¸é‡ {num_topics}: Score = {score:.4f}")

    print(f"\nâœ¨ æ¨è–¦çš„æœ€ä½³ä¸»é¡Œæ•¸é‡æ˜¯: {optimal_num_topics} (Coherence Score: {max_score:.4f})")

    plt.show()
