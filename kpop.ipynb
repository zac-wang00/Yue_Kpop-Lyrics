#英文原形處理
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f50e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# --- 檔案路徑設定 ---\n",
    "file_path = r\"C:\\Users\\lin\\Downloads\\python\\processed_SM_data.csv\"\n",
    "output_file_path = os.path.splitext(file_path)[0] + \"_lemmatized.csv\" # 輸出檔案名\n",
    "\n",
    "# --- 載入 SpaCy 英文模型 ---\n",
    "# 'en_core_web_sm' 是一個輕量且高效的英文模型\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy 英文模型 'en_core_web_sm' 未找到。請先執行:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    exit()\n",
    "\n",
    "def lemmatize_english_tokens(tokens_str):\n",
    "    \"\"\"\n",
    "    對字串形式的英文單詞列表進行詞形還原。\n",
    "    假設 tokens_str 是用空格分隔的單詞字串 (e.g., \"running cars were\")\n",
    "    \"\"\"\n",
    "    if not tokens_str:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. 處理輸入，將字串轉為 SpaCy Doc 物件\n",
    "    # 這裡我們信任輸入的 tokens_str 已經是分詞好的，但仍讓 SpaCy 處理以獲取詞元\n",
    "    doc = nlp(tokens_str)\n",
    "    \n",
    "    # 2. 執行詞形還原並保留非標點符號的詞元\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        # 僅保留有意義的詞元 (非空白、非標點符號)\n",
    "        if token.is_stop or token.is_punct or token.is_space or not token.lemma_.strip():\n",
    "            continue\n",
    "            \n",
    "        # 3. 避免還原結果與原始詞相同，但大小寫不同 (例如保持 'I' 而不是 'i')\n",
    "        # 如果詞元是 '-PRON-'，通常是代詞 (例如 'I', 'you')，我們保留原形\n",
    "        # 否則使用小寫詞元\n",
    "        lemma = token.lemma_ if token.lemma_ != '-PRON-' else token.text \n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    # 4. 將還原後的詞元列表重新組合為字串\n",
    "    return \" \".join(lemmas).lower() # 為了後續分析，統一轉為小寫\n",
    "\n",
    "# --- 主要執行區塊 ---\n",
    "try:\n",
    "    # 載入 CSV 檔案\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 檢查目標欄位是否存在\n",
    "    if 'english_tokens' not in df.columns:\n",
    "        print(f\"錯誤：CSV 檔案中找不到 'english_tokens' 欄位。\")\n",
    "        print(f\"現有欄位: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"檔案 '{file_path}' 載入成功，開始進行英文詞形還原...\")\n",
    "        \n",
    "        # 對 'english_tokens' 欄位應用詞形還原函數\n",
    "        # .fillna(\"\") 用於處理空值，避免錯誤\n",
    "        df['english_lemmas'] = df['english_tokens'].fillna(\"\").apply(lemmatize_english_tokens)\n",
    "\n",
    "        # 顯示前幾行結果以供檢查\n",
    "        print(\"\\n--- 詞形還原結果範例 ---\")\n",
    "        print(df[['english_tokens', 'english_lemmas']].head())\n",
    "        \n",
    "        # 儲存結果到新的 CSV 檔案\n",
    "        df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n詞形還原完成。結果已儲存至 '{output_file_path}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"錯誤：找不到檔案 '{file_path}'。請檢查路徑是否正確。\")\n",
    "except Exception as e:\n",
    "    print(f\"發生錯誤: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
